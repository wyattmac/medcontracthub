# Advanced HPA configurations with custom metrics for production

# OCR Service HPA with queue depth metrics
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ocr-service-hpa
  namespace: medcontract-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ocr-service
  minReplicas: 3
  maxReplicas: 20
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
      - type: Pods
        value: 1
        periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 4
        periodSeconds: 60
      selectPolicy: Max
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: External
    external:
      metric:
        name: kafka_consumer_lag
        selector:
          matchLabels:
            topic: "contracts.documents.events"
            consumer_group: "ocr-service-consumer"
      target:
        type: AverageValue
        averageValue: "1000"
  - type: External
    external:
      metric:
        name: redis_queue_length
        selector:
          matchLabels:
            queue: "ocr-processing"
      target:
        type: AverageValue
        averageValue: "100"

---
# AI Service HPA with inference latency metrics
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ai-service-hpa
  namespace: medcontract-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-service
  minReplicas: 5
  maxReplicas: 25
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 600  # Longer cooldown for AI models
      policies:
      - type: Percent
        value: 5
        periodSeconds: 120
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60  # Lower threshold for AI workloads
  - type: Pods
    pods:
      metric:
        name: ai_inference_duration_p95
      target:
        type: AverageValue
        averageValue: "2000m"  # 2 seconds p95 latency
  - type: External
    external:
      metric:
        name: weaviate_query_latency
        selector:
          matchLabels:
            service: "ai-service"
      target:
        type: AverageValue
        averageValue: "500m"  # 500ms query latency

---
# Analytics Service HPA with data ingestion rate
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: analytics-service-hpa
  namespace: medcontract-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: analytics-service
  minReplicas: 3
  maxReplicas: 15
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 75
  - type: External
    external:
      metric:
        name: kafka_incoming_messages_rate
        selector:
          matchLabels:
            topic: "contracts.*.events"
            consumer_group: "analytics-consumer-group"
      target:
        type: AverageValue
        averageValue: "1000"  # 1000 messages/sec per pod
  - type: External
    external:
      metric:
        name: clickhouse_insert_rate
        selector:
          matchLabels:
            database: "medcontracthub"
      target:
        type: AverageValue
        averageValue: "5000"  # 5000 rows/sec per pod

---
# Real-time Service HPA with WebSocket connections
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: realtime-service-hpa
  namespace: medcontract-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: realtime-service
  minReplicas: 5
  maxReplicas: 30
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Pods
        value: 5
        periodSeconds: 30
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50  # Lower for WebSocket overhead
  - type: Pods
    pods:
      metric:
        name: websocket_active_connections
      target:
        type: AverageValue
        averageValue: "200"  # 200 connections per pod
  - type: External
    external:
      metric:
        name: websocket_message_rate
        selector:
          matchLabels:
            service: "realtime-service"
      target:
        type: AverageValue
        averageValue: "1000"  # 1000 messages/sec per pod

---
# Worker Service HPA with job queue metrics
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: worker-service-hpa
  namespace: medcontract-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: worker-service
  minReplicas: 5
  maxReplicas: 50
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 20
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0  # Scale up immediately
      policies:
      - type: Percent
        value: 200
        periodSeconds: 15
  metrics:
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70
  - type: External
    external:
      metric:
        name: bullmq_queue_size
        selector:
          matchLabels:
            queue: "email-queue"
      target:
        type: AverageValue
        averageValue: "50"
  - type: External
    external:
      metric:
        name: bullmq_queue_size
        selector:
          matchLabels:
            queue: "sync-queue"
      target:
        type: AverageValue
        averageValue: "100"
  - type: External
    external:
      metric:
        name: job_processing_duration_p95
        selector:
          matchLabels:
            job_type: "sam_sync"
      target:
        type: AverageValue
        averageValue: "30s"

---
# Main App HPA with business metrics
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: medcontracthub-app-hpa
  namespace: medcontract-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: medcontracthub-app
  minReplicas: 5
  maxReplicas: 40
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 30
      - type: Pods
        value: 10
        periodSeconds: 30
      selectPolicy: Max
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 65
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 75
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  - type: External
    external:
      metric:
        name: api_response_time_p95
        selector:
          matchLabels:
            service: "medcontracthub-app"
      target:
        type: AverageValue
        averageValue: "500m"
  - type: External
    external:
      metric:
        name: active_user_sessions
        selector:
          matchLabels:
            app: "medcontracthub"
      target:
        type: AverageValue
        averageValue: "50"  # 50 active users per pod

---
# Cost-aware cluster autoscaler configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-status-configmap
  namespace: kube-system
data:
  nodes.max-node-provision-time: "15m"
  scale-down-delay-after-add: "10m"
  scale-down-unneeded-time: "10m"
  scale-down-utilization-threshold: "0.5"
  expander: "priority,least-waste"
  priority.yaml: |
    10:
      - m5.xlarge
      - m5.2xlarge
    20:
      - m5.4xlarge
      - m5.8xlarge
    30:
      - m5.12xlarge
    50:
      - p3.2xlarge  # GPU for AI workloads only

---
# Prometheus Adapter configuration for custom metrics
apiVersion: v1
kind: ConfigMap
metadata:
  name: adapter-config
  namespace: monitoring
data:
  config.yaml: |
    rules:
    - seriesQuery: 'kafka_consumer_lag_sum{job="kafka-exporter"}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
      name:
        matches: "^kafka_consumer_lag_sum"
        as: "kafka_consumer_lag"
      metricsQuery: 'avg_over_time(<<.Series>>{<<.LabelMatchers>>}[2m])'
    
    - seriesQuery: 'redis_queue_length{job="redis-exporter"}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
      name:
        matches: "^redis_queue_length"
        as: "redis_queue_length"
      metricsQuery: 'max_over_time(<<.Series>>{<<.LabelMatchers>>}[1m])'
    
    - seriesQuery: 'ai_inference_duration_seconds{quantile="0.95"}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod: {resource: "pod"}
      name:
        matches: "^ai_inference_duration_seconds"
        as: "ai_inference_duration_p95"
      metricsQuery: 'histogram_quantile(0.95, sum(rate(<<.Series>>{<<.LabelMatchers>>}[2m])) by (le))'
    
    - seriesQuery: 'websocket_active_connections{job="realtime-service"}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod: {resource: "pod"}
      name:
        matches: "^websocket_active_connections"
        as: "websocket_active_connections"
      metricsQuery: '<<.Series>>{<<.LabelMatchers>>}'
    
    - seriesQuery: 'bullmq_queue_size{job="worker-service"}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
      name:
        matches: "^bullmq_queue_size"
        as: "bullmq_queue_size"
      metricsQuery: 'max_over_time(<<.Series>>{<<.LabelMatchers>>}[1m])'
    
    - seriesQuery: 'http_requests_total{job="medcontracthub-app"}'
      resources:
        overrides:
          namespace: {resource: "namespace"}
          pod: {resource: "pod"}
      name:
        matches: "^http_requests_total"
        as: "http_requests_per_second"
      metricsQuery: 'rate(<<.Series>>{<<.LabelMatchers>>}[1m])'